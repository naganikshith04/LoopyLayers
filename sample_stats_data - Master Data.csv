Model,params_in_billion,params_per_token_bill,num_layers,size_in_gb,ctx_len,att_type,att_head_dim,model_architecture,hidden_dim,q_heads,k_heads,v_heads,supports_tool_calling,min_vram_in_gb,mmlu_5_shot_score,cache_type
meta-llama/Meta-Llama-3.1-8B-Instruct,8.03,8.03,32,16.07,128K,Group Query Attention,128,LlamaForCausalLM,4096,64,8,8,Y,16,66.70%,KVCache
Mistral-7B-instruct-v0.1,7.24,7.24,32,14.48,8k,"sliding window attention, Grouped Query attention",128,MistralForCausalLM,4096,32,8,8,N,16,62.50%,RollingBufferCache
Mixtral 8x7B-v0.1,46.7,12.9,32,96.8,32k,Grouped Query attention,128,mixture of experts(8 experts),4096,32,8,8,N,100,70.60%,KVCache
Mixtral 8x22B,140.6,39.1,56,290,64K,Grouped Query attention,128,mixture of experts(8 experts),6144,48,8,8,Y,300,77.75%,KVCache
Mistral Large 2,123,123,88,245,128k,Grouped Query attention,128,MistralForCausalLM,4096,32,8,8,Y,256,84%,KVCache